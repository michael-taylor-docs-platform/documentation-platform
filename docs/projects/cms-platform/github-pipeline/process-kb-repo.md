# Reusable Workflow: `process-kb-repo.yml`

**Parent Document:** [`GitHub KB Article Publishing Workflow`](./github-architecture.md)

This document provides a detailed breakdown of the `process-kb-repo.yml` reusable workflow. This workflow is a generic, reusable component responsible for processing a single Knowledge Base content repository.

## I. Purpose and Model

-   **Purpose:** To act as a standardized engine for processing article changes (updates and archives) for a specific audience. It encapsulates the steps required to check out the correct repositories, download the change data, and execute the specialist script that performs the core content transformation.
-   **Model:** This workflow is called by the main orchestrator (`elixir-kbarticle-dispatcher.yml`) for each audience that has changes (e.g., once for "Internal" and once for "Public").

## II. Inputs & Secrets

### Inputs

-   `repo-name` (string, required): The name of the content repository to process (e.g., `HIE-ELIXIR/KnowledgeBaseArticles-Internal`).
-   `audience` (string, required): The audience type (`internal` or `public`). This is used to determine which change data file to read.

### Secrets

This workflow requires a significant number of secrets to be passed down from the caller to facilitate authentication and communication with various services.

-   `AZURE_TENANT_ID`: The Azure AD Tenant ID.
-   `AZURE_CLIENT_ID`: The Client ID for the Azure AD service principal.
-   `AZURE_CLIENT_SECRET`: The Client Secret for the Azure AD service principal.
-   `DITA_CONVERTER_URL`: The URL for the `html-to-dita-converter` Azure Function.
-   `DITA_CONVERTER_FUNCTION_KEY`: The function key for the `html-to-dita-converter` Azure Function.
-   `HIE_SERVICE_GITHUB_TOKEN`: The service account token for authenticating `git` operations.

## III. Outputs

-   `has_failures` (boolean): Set to `true` if the specialist script recorded any errors while processing individual articles.
-   `processed_archives` (json): A JSON array of SharePoint IDs for archive-related articles that were processed.
-   `processed_updates` (json): A JSON array of SharePoint IDs for update-related articles that were processed *successfully*.
-   `commit_sha` (string): The final commit SHA of the content repository after all changes have been processed and committed.

## IV. Job Breakdown

The workflow consists of a single job, `process-repo`.

-   **Runner:** `ebf-pod-ubuntu-latest@${{ github.run_id }}-kb-${{ inputs.audience }}` (The runner is dynamically named based on the audience).
-   **Purpose:** To execute all the steps required for processing the repository.

### Steps:

1.  **`Checkout Workflow Repo`**: Checks out the main workflow repository (`KBConsolidation`) into a directory named `workflow-repo`. This is necessary to gain access to the composite actions and scripts it contains.

2.  **`Get Microsoft Graph API token`**: Calls the reusable `.github/actions/get-graph-token` composite action (located in the `workflow-repo` checkout) to acquire a Graph API token. This token is passed as an environment variable to the specialist script.

3.  **`Checkout Target KB Repo`**: Checks out the specific content repository that needs to be processed.
    -   The `repository` is specified by the `inputs.repo-name`.
    -   The code is checked out into the `kb-repo` directory.
    -   Authentication is handled by the `secrets.HIE_SERVICE_GITHUB_TOKEN`.

4.  **`Download Change Data`**: Downloads the `change-data` artifact, which was uploaded by the `gather-and-sort-changes` job in the orchestrating workflow.

5.  **`Make script executable`**: Runs `chmod +x` on the `process-articles.sh` script to ensure it has the necessary permissions to be executed.

6.  **`Process Changes`**: This is the core step of the job.
    -   **Working Directory:** It changes into the `kb-repo` directory, so all script operations are relative to the content repository.
    -   **Environment Variables:** It sets several environment variables that the `process-articles.sh` script will use:
        -   `GRAPH_API_TOKEN`: The Graph API token from the earlier step.
        -   `DITA_CONVERTER_URL`: The URL for the DITA conversion function.
        -   `DITA_CONVERTER_FUNCTION_KEY`: The key for the DITA conversion function.
        -   `AUDIENCE`: The audience input for this workflow run.
    -   **Execution:**
        -   It reads the content of the correct changes file (e.g., `../change-data/internal-changes.json`) into an environment variable named `JSON_INPUT`.
        -   It then executes the `process-articles.sh` script.

7.  **`Upload Processing Results`**: After the script finishes, it uploads a file named `processing-results.json` (which is generated by the script) as an artifact. The artifact is dynamically named based on the audience (e.g., `internal-processing-results`). This file contains the granular success/failure status for each article processed.
